{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKpTeoSgQfPJe1W3o+I+Q3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishra-yogendra/DSPy-Practical-Assignment/blob/main/DSPy_Practical_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DSPy Entity Extraction & Knowledge Graph Generator\n",
        "##Assignment: Structuring Unstructured Data with LLMs\n",
        "\n",
        "###This notebook demonstrates:\n",
        "###1. Entity extraction from web content\n",
        "###2. Intelligent deduplication with confidence loops\n",
        "###3. Mermaid knowledge graph generation\n",
        "###4. CSV export of structured data\n"
      ],
      "metadata": {
        "id": "sny4O-Z_uCjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dspy\n",
        "import dspy\n",
        "from dspy import Predict, ChainOfThought\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp-7lQUZttxK",
        "outputId": "eedcd1e5-5ca9-4871-d913-4f17c1eaada7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy\n",
            "  Downloading dspy-3.0.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting backoff>=2.2 (from dspy)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy) (1.5.2)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from dspy) (1.109.1)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from dspy) (2024.11.6)\n",
            "Requirement already satisfied: orjson>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (3.11.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from dspy) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (2.32.4)\n",
            "Collecting optuna>=3.4.0 (from dspy)\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (2.11.10)\n",
            "Collecting magicattr>=0.1.6 (from dspy)\n",
            "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting litellm>=1.64.0 (from dspy)\n",
            "  Downloading litellm-1.79.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting diskcache>=5.6.0 (from dspy)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting json-repair>=0.30.0 (from dspy)\n",
            "  Downloading json_repair-0.52.5-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from dspy) (8.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy) (4.11.0)\n",
            "Collecting asyncer==0.0.8 (from dspy)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (3.1.2)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from dspy) (13.9.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (2.0.2)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy) (3.6.0)\n",
            "Collecting gepa==0.0.7 (from gepa[dspy]==0.0.7->dspy)\n",
            "  Downloading gepa-0.0.7-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy) (4.15.0)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (3.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (8.3.0)\n",
            "Collecting fastuuid>=0.13.0 (from litellm>=1.64.0->dspy)\n",
            "  Downloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (4.25.1)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (1.2.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (0.12.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy) (0.22.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy) (0.11.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy) (1.17.1)\n",
            "Collecting colorlog (from optuna>=3.4.0->dspy)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy) (2.0.44)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy) (1.22.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy) (1.3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm>=1.64.0->dspy) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.64.0->dspy) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.64.0->dspy) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.64.0->dspy) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy) (0.28.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy) (0.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm>=1.64.0->dspy) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy) (1.2.0)\n",
            "Downloading dspy-3.0.3-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading gepa-0.0.7-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.52.5-py3-none-any.whl (26 kB)\n",
            "Downloading litellm-1.79.1-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: magicattr, json-repair, gepa, fastuuid, diskcache, colorlog, backoff, asyncer, optuna, litellm, dspy\n",
            "Successfully installed asyncer-0.0.8 backoff-2.2.1 colorlog-6.10.1 diskcache-5.6.3 dspy-3.0.3 fastuuid-0.14.0 gepa-0.0.7 json-repair-0.52.5 litellm-1.79.1 magicattr-0.1.6 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION\n",
        "\n",
        "API_KEY = \"YOUR_GROQ_API_KEY\"\n",
        "\n",
        "# Configure DSPy with Groq AI\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = API_KEY\n",
        "\n",
        "try:\n",
        "    # Groq AI configuration - Fast inference with open source models\n",
        "    lm = dspy.LM(\n",
        "        model=\"groq/llama-3.3-70b-versatile\",  # Best balance of speed and quality\n",
        "        api_key=API_KEY,\n",
        "        api_base=\"https://api.groq.com/openai/v1\",\n",
        "        temperature=0.3,  # Lower temperature for consistent extraction\n",
        "        max_tokens=8000\n",
        "    )\n",
        "    dspy.configure(lm=lm)\n",
        "    print(\"‚úì DSPy configured with Groq AI\")\n",
        "    print(f\"  Model: llama-3.3-70b-versatile\")\n",
        "except Exception as e:\n",
        "    print(f\"Configuration failed: {e}\")\n",
        "    print(\"Please check your Groq API key\")\n",
        "\n",
        "\n",
        "# URLs to process\n",
        "URLS = [\n",
        "    'https://en.wikipedia.org/wiki/Sustainable_agriculture',\n",
        "    'https://www.nature.com/articles/d41586-025-03353-5',\n",
        "    'https://www.sciencedirect.com/science/article/pii/S1043661820315152',\n",
        "    'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/',\n",
        "    'https://www.fao.org/3/y4671e/y4671e06.htm',\n",
        "    'https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria',\n",
        "    'https://www.sciencedirect.com/science/article/pii/S0378378220307088',\n",
        "    'https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets',\n",
        "    'https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7',\n",
        "    'https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india'\n",
        "]\n",
        "\n",
        "\n",
        "# PYDANTIC MODELS FOR STRUCTURED OUTPUT\n",
        "\n",
        "class EntityWithAttr(BaseModel):\n",
        "    \"\"\"Structured entity with semantic type\"\"\"\n",
        "    entity: str = Field(description=\"the named entity\")\n",
        "    attr_type: str = Field(description=\"semantic type (e.g. Drug, Disease, Concept, Method, Process)\")\n",
        "\n",
        "\n",
        "class EntityList(BaseModel):\n",
        "    \"\"\"List of entities extracted from text\"\"\"\n",
        "    entities: List[EntityWithAttr]\n",
        "\n",
        "\n",
        "class DeduplicatedEntities(BaseModel):\n",
        "    \"\"\"Deduplicated list of entities\"\"\"\n",
        "    deduplicated: List[EntityWithAttr]\n",
        "    confidence: float = Field(description=\"confidence score 0-1\")\n",
        "\n",
        "\n",
        "class RelationTriple(BaseModel):\n",
        "    \"\"\"Relationship between two entities\"\"\"\n",
        "    source: str\n",
        "    relation: str\n",
        "    target: str\n",
        "\n",
        "\n",
        "class RelationList(BaseModel):\n",
        "    \"\"\"List of relationships\"\"\"\n",
        "    triples: List[RelationTriple]\n",
        "\n",
        "\n",
        "# DSPY SIGNATURES\n",
        "\n",
        "class ExtractEntities(dspy.Signature):\n",
        "    \"\"\"Extract named entities and their semantic types from text\"\"\"\n",
        "    paragraph: str = dspy.InputField()\n",
        "    entities: List[EntityWithAttr] = dspy.OutputField()\n",
        "\n",
        "\n",
        "class DeduplicateEntities(dspy.Signature):\n",
        "    \"\"\"Deduplicate similar entities (e.g., 'PB IC', 'pea-barley intercrop' -> 1 entity)\"\"\"\n",
        "    items: str = dspy.InputField(desc=\"list of entities to deduplicate\")\n",
        "    deduplicated: List[str] = dspy.OutputField()\n",
        "    confidence: float = dspy.OutputField(desc=\"confidence score 0-1\")\n",
        "\n",
        "\n",
        "class ExtractRelations(dspy.Signature):\n",
        "    \"\"\"Extract relationships between entities\"\"\"\n",
        "    text: str = dspy.InputField()\n",
        "    entities: str = dspy.InputField(desc=\"list of valid entities\")\n",
        "    triples: List[RelationTriple] = dspy.OutputField()\n",
        "\n",
        "\n",
        "# UTILITY FUNCTIONS\n",
        "\n",
        "def scrape_text(url: str, max_chars: int = 5000) -> str:\n",
        "    \"\"\"Scrape and clean text from URL\"\"\"\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        # Get text\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "        # Clean text\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text[:max_chars]  # Limit length\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def clean_entity_name(name: str) -> str:\n",
        "    \"\"\"Clean entity name for Mermaid graph\"\"\"\n",
        "    # Remove special characters, keep alphanumeric and spaces\n",
        "    cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', name)\n",
        "    # Replace spaces with underscores\n",
        "    cleaned = re.sub(r'\\s+', '_', cleaned)\n",
        "    return cleaned[:50]  # Limit length\n",
        "\n",
        "\n",
        "# MAIN PROCESSING FUNCTIONS\n",
        "\n",
        "def extract_entities_from_text(text: str) -> List[EntityWithAttr]:\n",
        "    \"\"\"Extract entities using DSPy predictor\"\"\"\n",
        "    predictor = Predict(ExtractEntities)\n",
        "\n",
        "    # Split text into chunks if too long\n",
        "    chunks = [text[i:i+2000] for i in range(0, min(len(text), 6000), 2000)]\n",
        "\n",
        "    all_entities = []\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        try:\n",
        "            print(f\"Processing chunk {idx + 1}/{len(chunks)}...\")\n",
        "            result = predictor(paragraph=chunk)\n",
        "            if hasattr(result, 'entities') and result.entities:\n",
        "                all_entities.extend(result.entities)\n",
        "                print(f\"Found {len(result.entities)} entities in chunk {idx + 1}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chunk {idx + 1}: {str(e)[:100]}\")\n",
        "            # Try simpler extraction as fallback\n",
        "            try:\n",
        "                # Use CoT for better results\n",
        "                cot_predictor = ChainOfThought(ExtractEntities)\n",
        "                result = cot_predictor(paragraph=chunk[:1500])\n",
        "                if hasattr(result, 'entities') and result.entities:\n",
        "                    all_entities.extend(result.entities)\n",
        "                    print(f\"CoT found {len(result.entities)} entities in chunk {idx + 1}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"CoT also failed: {str(e2)[:100]}\")\n",
        "                continue\n",
        "\n",
        "    return all_entities\n",
        "\n",
        "\n",
        "def deduplicate_with_lm(items: List[EntityWithAttr], target_confidence: float = 0.8) -> List[EntityWithAttr]:\n",
        "    \"\"\"Deduplicate entities with confidence loop\"\"\"\n",
        "    predictor = Predict(DeduplicateEntities)\n",
        "\n",
        "    # Convert entities to string format\n",
        "    items_str = \"\\n\".join([f\"{e.entity} ({e.attr_type})\" for e in items])\n",
        "\n",
        "    max_attempts = 3\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            result = predictor(items=items_str)\n",
        "\n",
        "            # Check confidence\n",
        "            confidence = getattr(result, 'confidence', 0.5)\n",
        "            if confidence >= target_confidence or attempt == max_attempts - 1:\n",
        "                # Parse deduplicated results\n",
        "                deduped = []\n",
        "                for orig_entity in items:\n",
        "                    # Check if entity is in deduplicated list\n",
        "                    if any(orig_entity.entity.lower() in d.lower() for d in result.deduplicated):\n",
        "                        deduped.append(orig_entity)\n",
        "\n",
        "                # Remove exact duplicates\n",
        "                seen = set()\n",
        "                unique_deduped = []\n",
        "                for e in deduped:\n",
        "                    key = (e.entity.lower(), e.attr_type.lower())\n",
        "                    if key not in seen:\n",
        "                        seen.add(key)\n",
        "                        unique_deduped.append(e)\n",
        "\n",
        "                return unique_deduped\n",
        "        except Exception as e:\n",
        "            print(f\"Deduplication attempt {attempt + 1} failed: {e}\")\n",
        "\n",
        "    # Fallback: manual deduplication\n",
        "    seen = set()\n",
        "    unique = []\n",
        "    for e in items:\n",
        "        key = (e.entity.lower(), e.attr_type.lower())\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique.append(e)\n",
        "\n",
        "    return unique\n",
        "\n",
        "\n",
        "def extract_relations(text: str, entities: List[EntityWithAttr]) -> List[RelationTriple]:\n",
        "    \"\"\"Extract relationships between entities\"\"\"\n",
        "    predictor = Predict(ExtractRelations)\n",
        "\n",
        "    entity_list = \", \".join([e.entity for e in entities])\n",
        "\n",
        "    try:\n",
        "        result = predictor(text=text[:2000], entities=entity_list)\n",
        "        if hasattr(result, 'triples'):\n",
        "            return result.triples\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting relations: {e}\")\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "def generate_mermaid_diagram(entities: List[EntityWithAttr], triples: List[RelationTriple]) -> str:\n",
        "    \"\"\"Generate Mermaid diagram from entities and relations\"\"\"\n",
        "    mermaid = \"graph TD\\n\"\n",
        "\n",
        "    # Create entity set for validation\n",
        "    entity_set = {e.entity.lower() for e in entities}\n",
        "\n",
        "    # Add nodes with types\n",
        "    for entity in entities:\n",
        "        clean_id = clean_entity_name(entity.entity)\n",
        "        label = entity.entity[:40]  # Limit label length\n",
        "        mermaid += f'  {clean_id}[\"{label}<br/><i>{entity.attr_type}</i>\"]\\n'\n",
        "\n",
        "    # Add edges\n",
        "    added_edges = set()\n",
        "    for triple in triples:\n",
        "        # Validate entities exist\n",
        "        if triple.source.lower() in entity_set and triple.target.lower() in entity_set:\n",
        "            src_clean = clean_entity_name(triple.source)\n",
        "            dst_clean = clean_entity_name(triple.target)\n",
        "            label = triple.relation[:40]  # Limit label length\n",
        "\n",
        "            edge_key = (src_clean, dst_clean, label)\n",
        "            if edge_key not in added_edges:\n",
        "                mermaid += f'  {src_clean} -- \"{label}\" --> {dst_clean}\\n'\n",
        "                added_edges.add(edge_key)\n",
        "\n",
        "    return mermaid\n",
        "\n",
        "\n",
        "# MAIN PIPELINE\n",
        "\n",
        "def process_url(url: str, index: int):\n",
        "    \"\"\"Process a single URL through the complete pipeline\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing URL {index + 1}/10: {url}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Step 1: Scrape text\n",
        "    print(\"Step 1: Scraping content...\")\n",
        "    text = scrape_text(url)\n",
        "    if not text:\n",
        "        print(\"Failed to scrape content\")\n",
        "        return None\n",
        "    print(f\"Scraped {len(text)} characters\")\n",
        "\n",
        "    # Step 2: Extract entities\n",
        "    print(\"Step 2: Extracting entities...\")\n",
        "    entities = extract_entities_from_text(text)\n",
        "    print(f\"Extracted {len(entities)} entities\")\n",
        "\n",
        "    # Step 3: Deduplicate\n",
        "    print(\"Step 3: Deduplicating entities...\")\n",
        "    deduped_entities = deduplicate_with_lm(entities)\n",
        "    print(f\"Deduplicated to {len(deduped_entities)} unique entities\")\n",
        "\n",
        "    # Step 4: Extract relations\n",
        "    print(\"Step 4: Extracting relationships...\")\n",
        "    relations = extract_relations(text, deduped_entities)\n",
        "    print(f\"Extracted {len(relations)} relationships\")\n",
        "\n",
        "    # Step 5: Generate Mermaid diagram\n",
        "    print(\"Step 5: Generating Mermaid diagram...\")\n",
        "    mermaid = generate_mermaid_diagram(deduped_entities, relations)\n",
        "    print(f\"Generated diagram\")\n",
        "\n",
        "    return {\n",
        "        'url': url,\n",
        "        'entities': deduped_entities,\n",
        "        'relations': relations,\n",
        "        'mermaid': mermaid,\n",
        "        'index': index + 1\n",
        "    }\n",
        "\n",
        "\n",
        "def process_url_with_manual_text(url: str, text: str, index: int):\n",
        "    \"\"\"Process a URL with pre-extracted text instead of scraping\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing URL {index + 1} (manual text): {url}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Skip scraping, use provided text\n",
        "    print(f\"Step 1: Using provided text ({len(text)} characters)\")\n",
        "\n",
        "    # Step 2: Extract entities\n",
        "    print(\"Step 2: Extracting entities...\")\n",
        "    entities = extract_entities_from_text(text)\n",
        "    print(f\"Extracted {len(entities)} entities\")\n",
        "\n",
        "    # Step 3: Deduplicate\n",
        "    print(\"Step 3: Deduplicating entities...\")\n",
        "    deduped_entities = deduplicate_with_lm(entities)\n",
        "    print(f\"Deduplicated to {len(deduped_entities)} unique entities\")\n",
        "\n",
        "    # Step 4: Extract relations\n",
        "    print(\"Step 4: Extracting relationships...\")\n",
        "    relations = extract_relations(text, deduped_entities)\n",
        "    print(f\"Extracted {len(relations)} relationships\")\n",
        "\n",
        "    # Step 5: Generate Mermaid diagram\n",
        "    print(\"Step 5: Generating Mermaid diagram...\")\n",
        "    mermaid = generate_mermaid_diagram(deduped_entities, relations)\n",
        "    print(f\"Generated diagram\")\n",
        "\n",
        "    return {\n",
        "        'url': url,\n",
        "        'entities': deduped_entities,\n",
        "        'relations': relations,\n",
        "        'mermaid': mermaid,\n",
        "        'index': index + 1\n",
        "    }\n",
        "\n",
        "\n",
        "# MANUAL TEXT DEFINITIONS\n",
        "text_url1 = \"\"\"\n",
        "Ivermectin is a macrolide antiparasitic drug ... Recently, ivermectin has been reported to inhibit the proliferation of several tumor cells by regulating multiple signaling pathways. This suggests that ivermectin may be an anticancer drug with great potential. Here, we reviewed the related mechanisms by which ivermectin inhibited the development of different cancers and promoted programmed cell death and discussed the prospects for the clinical application of ivermectin as an anticancer drug for neoplasm therapy.\n",
        "\n",
        "Keywords: Ivermectin, avermectin, selamectin, doramectin, moxidectin, cancer, tumor, neoplasm, triple-negative breast cancer (TNBC), drug repositioning, apoptosis, autophagy, pyroptosis, proliferation, metastasis, angiogenic activity, multidrug resistance (MDR), cell death, PAK1 kinase, EGFR, HER2, ASC, GSDMD, LDH, PARP, P-glycoprotein (P-gp), SOX-2, OCT-4, STAT3, YAP1, HMGB1, HSP27, signaling pathways, crosstalk, chemotherapy drugs, targeted drugs.\n",
        "\"\"\"\n",
        "\n",
        "text_url2 = \"\"\"\n",
        "There is a significant relationship between ambient temperature and mortality ... in vulnerable groups, especially in elderly over the age of 65 years, infants and individuals with co-morbid cardiovascular and/or respiratory conditions, there is a deficiency in thermoregulation. When temperatures exceed a certain limit, being cold winter spells or heat waves, there is an increase in the number of deaths. ... Besides the direct effect of temperature rises on human health, global warming will have a negative impact on primary producers and livestock, leading to malnutrition ... Public health measures ... improved urban planning and reduction in energy consumption ... reduce the carbon footprint and help avert global warming, thus reducing mortality.\n",
        "\n",
        "Keywords: global warming, ambient temperature, thermoregulation, heat regulation, heat waves, cold spells, mortality, malnutrition, carbon footprint, urban planning, energy consumption, climate change, air pollution, greenhouse effect, cardiovascular, respiratory disease, vector-borne diseases, waterborne diseases, foodborne diseases, mental health problems, allergies, elderly, infants, primary producers, livestock, vulnerable groups, air pollution, carbon dioxide, methane, nitrous oxide, ocean acidification, ozone depletion, adaptation, acclimatization, epidemiological studies, public health measures.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Starting DSPy Entity Extraction Pipeline\")\n",
        "    print(f\"Processing {len(URLS)} URLs...\\n\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "\n",
        "    # Process all URLs\n",
        "    results = []\n",
        "    csv_rows = [['link', 'tag', 'tag_type']]\n",
        "\n",
        "    # Dictionary for manual text overrides\n",
        "    manual_texts = {\n",
        "        'https://www.sciencedirect.com/science/article/pii/S1043661820315152': text_url1,\n",
        "        'https://www.sciencedirect.com/science/article/pii/S0378378220307088': text_url2\n",
        "    }\n",
        "\n",
        "    for i, url in enumerate(URLS):\n",
        "        # Check if we have manual text for this URL\n",
        "        if url in manual_texts:\n",
        "            result = process_url_with_manual_text(url, manual_texts[url], i)\n",
        "        else:\n",
        "            result = process_url(url, i)\n",
        "\n",
        "        if result:\n",
        "            results.append(result)\n",
        "\n",
        "            # Save Mermaid diagram\n",
        "            mermaid_file = f'output/mermaid_{result[\"index\"]}.md'\n",
        "            with open(mermaid_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(result['mermaid'])\n",
        "            print(f\"Saved {mermaid_file}\")\n",
        "\n",
        "            # Add to CSV\n",
        "            for entity in result['entities']:\n",
        "                csv_rows.append([url, entity.entity, entity.attr_type])\n",
        "\n",
        "    # Save CSV\n",
        "    df = pd.DataFrame(csv_rows[1:], columns=csv_rows[0])\n",
        "    csv_file = 'output/tags.csv'\n",
        "    df.to_csv(csv_file, index=False)\n",
        "    print(f\"\\n‚úì Saved {csv_file} with {len(df)} rows\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úì PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Processed: {len(results)}/{len(URLS)} URLs\")\n",
        "    print(f\"Total entities: {sum(len(r['entities']) for r in results)}\")\n",
        "    print(f\"Total relationships: {sum(len(r['relations']) for r in results)}\")\n",
        "    print(f\"\\nOutputs saved to 'output/' directory:\")\n",
        "    print(f\"  - mermaid_1.md to mermaid_10.md\")\n",
        "    print(f\"  - tags.csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test API connection first\n",
        "    print(\"üîç Testing API connection...\")\n",
        "    try:\n",
        "        test_predictor = Predict(\"question -> answer\")\n",
        "        test_result = test_predictor(question=\"What is 2+2?\")\n",
        "        print(f\"‚úì API connection successful! Test response: {test_result.answer}\")\n",
        "        print(\"\\nStarting main pipeline...\\n\")\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå API connection failed: {e}\")\n",
        "        print(\"\\nüîß Troubleshooting steps:\")\n",
        "        print(\"1. Check your API key is correct\")\n",
        "        print(\"2. Verify Groq endpoint: https://api.groq.com/openai/v1\")\n",
        "        print(\"3. Check your quota at Groq dashboard\")\n",
        "        print(\"4. Try alternative configuration:\")\n",
        "        print(\"\\n   # Option A: Use environment variables\")\n",
        "        print(\"   export GROQ_API_KEY='your-key'\")\n",
        "        print(\"\\n   # Option B: Try different model\")\n",
        "        print(\"   lm = dspy.LM(model='groq/llama-3.1-70b-versatile', api_key=API_KEY, api_base='...')\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDHuMyft29tr",
        "outputId": "7fe837b5-6c0d-4cb8-ab69-a20049faef52"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì DSPy configured with Groq AI\n",
            "  Model: llama-3.3-70b-versatile\n",
            "üîç Testing API connection...\n",
            "‚úì API connection successful! Test response: The answer to 2+2 is 4.\n",
            "\n",
            "Starting main pipeline...\n",
            "\n",
            "Starting DSPy Entity Extraction Pipeline\n",
            "Processing 10 URLs...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Processing URL 1/10: https://en.wikipedia.org/wiki/Sustainable_agriculture\n",
            "================================================================================\n",
            "Step 1: Scraping content...\n",
            "Scraped 5000 characters\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/3...\n",
            "Found 12 entities in chunk 1\n",
            "Processing chunk 2/3...\n",
            "Found 68 entities in chunk 2\n",
            "Processing chunk 3/3...\n",
            "Found 38 entities in chunk 3\n",
            "Extracted 118 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 106 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 10 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_1.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 2/10: https://www.nature.com/articles/d41586-025-03353-5\n",
            "================================================================================\n",
            "Step 1: Scraping content...\n",
            "Scraped 5000 characters\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/3...\n",
            "Found 6 entities in chunk 1\n",
            "Processing chunk 2/3...\n",
            "Found 19 entities in chunk 2\n",
            "Processing chunk 3/3...\n",
            "Found 10 entities in chunk 3\n",
            "Extracted 35 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 32 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 6 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_2.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 3 (manual text): https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "================================================================================\n",
            "Step 1: Using provided text (975 characters)\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/1...\n",
            "Found 36 entities in chunk 1\n",
            "Extracted 36 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 35 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 25 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_3.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 4/10: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\n",
            "================================================================================\n",
            "Step 1: Scraping content...\n",
            "Scraped 5000 characters\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/3...\n",
            "Found 12 entities in chunk 1\n",
            "Processing chunk 2/3...\n",
            "Found 17 entities in chunk 2\n",
            "Processing chunk 3/3...\n",
            "Found 22 entities in chunk 3\n",
            "Extracted 51 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 49 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 9 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_4.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 5/10: https://www.fao.org/3/y4671e/y4671e06.htm\n",
            "================================================================================\n",
            "Step 1: Scraping content...\n",
            "Scraped 5000 characters\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/3...\n",
            "Found 10 entities in chunk 1\n",
            "Processing chunk 2/3...\n",
            "Found 9 entities in chunk 2\n",
            "Processing chunk 3/3...\n",
            "Found 12 entities in chunk 3\n",
            "Extracted 31 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 26 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 15 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_5.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 6/10: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
            "================================================================================\n",
            "Step 1: Scraping content...\n",
            "Scraped 5000 characters\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/3...\n",
            "Found 15 entities in chunk 1\n",
            "Processing chunk 2/3...\n",
            "Found 17 entities in chunk 2\n",
            "Processing chunk 3/3...\n",
            "Found 12 entities in chunk 3\n",
            "Extracted 44 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 40 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 11 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_6.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 7 (manual text): https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "================================================================================\n",
            "Step 1: Using provided text (1370 characters)\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/1...\n",
            "Found 36 entities in chunk 1\n",
            "Extracted 36 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 33 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 21 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_7.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 8/10: https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\n",
            "================================================================================\n",
            "Step 1: Scraping content...\n",
            "Scraped 5000 characters\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/3...\n",
            "Found 10 entities in chunk 1\n",
            "Processing chunk 2/3...\n",
            "Found 10 entities in chunk 2\n",
            "Processing chunk 3/3...\n",
            "Found 11 entities in chunk 3\n",
            "Extracted 31 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 29 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 8 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_8.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 9/10: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
            "================================================================================\n",
            "Step 1: Scraping content...\n",
            "Scraped 5000 characters\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/3...\n",
            "Found 10 entities in chunk 1\n",
            "Processing chunk 2/3...\n",
            "Found 18 entities in chunk 2\n",
            "Processing chunk 3/3...\n",
            "Found 12 entities in chunk 3\n",
            "Extracted 40 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 37 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 8 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_9.md\n",
            "\n",
            "================================================================================\n",
            "Processing URL 10/10: https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\n",
            "================================================================================\n",
            "Step 1: Scraping content...\n",
            "Scraped 5000 characters\n",
            "Step 2: Extracting entities...\n",
            "Processing chunk 1/3...\n",
            "Found 12 entities in chunk 1\n",
            "Processing chunk 2/3...\n",
            "Found 11 entities in chunk 2\n",
            "Processing chunk 3/3...\n",
            "Found 7 entities in chunk 3\n",
            "Extracted 30 entities\n",
            "Step 3: Deduplicating entities...\n",
            "Deduplicated to 25 unique entities\n",
            "Step 4: Extracting relationships...\n",
            "Extracted 9 relationships\n",
            "Step 5: Generating Mermaid diagram...\n",
            "Generated diagram\n",
            "Saved output/mermaid_10.md\n",
            "\n",
            "‚úì Saved output/tags.csv with 412 rows\n",
            "\n",
            "================================================================================\n",
            "‚úì PIPELINE COMPLETE!\n",
            "================================================================================\n",
            "Processed: 10/10 URLs\n",
            "Total entities: 412\n",
            "Total relationships: 122\n",
            "\n",
            "Outputs saved to 'output/' directory:\n",
            "  - mermaid_1.md to mermaid_10.md\n",
            "  - tags.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9zxpIIfl3i15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}